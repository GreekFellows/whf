import re

# enumeration code from
# http://stackoverflow.com/a/1695250
def enum(*sequential, **named):
	enums = dict(zip(sequential, range(len(sequential))), **named)
	return type('Enum', (), enums)

# token tags, correspond to our data types as well

token_tags = enum(
	# the 2 only types
	"INT", "STR",

	# keywords
	# string manipulation
	"HAN", "HANG",

	# i/o
	"NG"
	)

token_tags_dict = {
	token_tags.INT: "int",
	token_tags.STR: "str",
	token_tags.HAN: "han",
	token_tags.HANG: "hang",
	token_tags.NG: "ng"
}

# types

class whf_type(object):
	def __init__(self, tag):
		self.tag = tag

	def __eq__(self, other):
		try:
			return (type(self) == type(other)) and (self.tag == other.tag)
		except:
			return False

	def __repr__(self):
		return "whf_type(" + token_tags_dict[self.tag] + ")"

	def __hash__(self):
		return hash(self.tag)

	@staticmethod
	def is_numeric(what):
		return what.tag == token_tags.INT

	@staticmethod
	def is_text(what):
		return what.tag == token_tags.STR

whf_type.int_type = whf_type(token_tags.INT)
whf_type.str_type = whf_type(token_tags.STR)

# tokens, generated by the lexer function

class token_info(object):
	def __init__(self, start, end, lexeme):
		self.start = start
		self.end = end
		self.lexeme = lexeme

	def __repr__(self):
		return "token_info(" + str(self.start) + ", " + str(self.end) + ", " + self.lexeme + ")"

class token(object):
	def __init__(self, tag, info):
		self.tag = tag
		self.info = info

	def __repr__(self):
		return "token(" + token_tags_dict[self.tag] + ", " + repr(self.info) + ")"

class int_token(token):
	def __init__(self, value, info):
		super().__init__(token_tags.INT, info)
		self.value = value
		self.type = whf_type.int_type

	def __repr__(self):
		return "int_token(" + str(self.value) + ", " + repr(self.info) + ")"

class str_token(token):
	def __init__(self, value, info):
		super().__init__(token_tags.STR, info)
		self.value = value
		self.type = whf_type.str_type

	def __repr__(self):
		return "str_token(" + value + ", " + repr(self.info) + ")"

class word_token(token):
	def __init__(self, tag, info):
		super().__init__(tag, info)

	def __repr__(self):
		return "word_token(" + token_tags_dict[self.tag] + ", " + repr(self.info) + ")"

# lexer function

def lex(s):
	tokens = []

	whspace_pattern = "( |\t|\n|\r)+"
	int_lit_pattern = "W[weihangf]*"

	whspace_prog = re.compile(whspace_pattern)
	int_lit_prog = re.compile(int_lit_pattern)

	while (len(s) > 0):
		# ignore whitespaces
		whspace_match = whspace_prog.match(s)
		if (whspace_match):
			s = s[whspace_match.end():]
			continue

		# int literals
		int_lit_match = int_lit_prog.match(s)
		if (int_lit_match):
			int_lit_start = int_lit_match.start()
			int_lit_end = int_lit_match.end()
			int_lit_lexeme = int_lit_match.group(0)

			tok = int_token(
				int(int_lit_end - int_lit_start - 1),
				token_info(
					int_lit_start,
					int_lit_end,
					int_lit_lexeme
					)
				)
			tokens.append(tok)

			s = s[int_lit_end:]
			continue

		# error
		raise Exception("lexer error")

	return tokens

def main(s):
	tokens = lex(s)
	for token in tokens:
		print(token)