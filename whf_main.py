import re

# enumeration code from
# http://stackoverflow.com/a/1695250
def enum(*sequential, **named):
	enums = dict(zip(sequential, range(len(sequential))), **named)
	return type('Enum', (), enums)

# token tags, correspond to our data types as well

token_tags = enum(
	# generic stuff
	"LP", "RP",

	# the 2 only types
	"INT", "STR",

	# keywords
	# string manipulation
	"H",

	# i/o
	"NG"
	)

token_tags_dict = {
	token_tags.LP: "(",
	token_tags.RP: ")",
	token_tags.INT: "int",
	token_tags.STR: "str",
	token_tags.H: "h",
	token_tags.NG: "ng"
}

kws_dict = {
	"h": token_tags.H,
	"ng": token_tags.NG
}

# types

class whf_type(object):
	def __init__(self, tag):
		self.tag = tag

	def __eq__(self, other):
		try:
			return (type(self) == type(other)) and (self.tag == other.tag)
		except:
			return False

	def __repr__(self):
		return "whf_type(" + token_tags_dict[self.tag] + ")"

	def __hash__(self):
		return hash(self.tag)

	@staticmethod
	def is_numeric(what):
		return what.tag == token_tags.INT

	@staticmethod
	def is_text(what):
		return what.tag == token_tags.STR

whf_type.int_type = whf_type(token_tags.INT)
whf_type.str_type = whf_type(token_tags.STR)

# tokens, generated by the lexer function

class token_info(object):
	def __init__(self, start, end, lexeme):
		self.start = start
		self.end = end
		self.lexeme = lexeme

	def __repr__(self):
		return "token_info(" + str(self.start) + ", " + str(self.end) + ", " + self.lexeme + ")"

class token(object):
	def __init__(self, tag, info):
		self.tag = tag
		self.info = info

	def __repr__(self):
		return "token(" + token_tags_dict[self.tag] + ", " + repr(self.info) + ")"

class int_token(token):
	def __init__(self, value, info):
		super().__init__(token_tags.INT, info)
		self.value = value
		self.type = whf_type.int_type

	def __repr__(self):
		return "int_token(" + str(self.value) + ", " + repr(self.info) + ")"

class str_token(token):
	def __init__(self, value, info):
		super().__init__(token_tags.STR, info)
		self.value = value
		self.type = whf_type.str_type

	def __repr__(self):
		return "str_token(" + value + ", " + repr(self.info) + ")"

class word_token(token):
	def __init__(self, tag, info):
		super().__init__(tag, info)

	def __repr__(self):
		return "word_token(" + token_tags_dict[self.tag] + ", " + repr(self.info) + ")"

# lexer function and helper functions

def find_kw_token_tag(lexeme):
	return kws_dict.get(lexeme)

def lex(s):
	i = 0
	tokens = []

	whspace_pattern = "( |\t|\n|\r)+"
	int_lit_pattern = "W[weihangf]*"
	word_pattern = "[weihangf]+"

	whspace_prog = re.compile(whspace_pattern)
	int_lit_prog = re.compile(int_lit_pattern)
	word_prog = re.compile(word_pattern)

	while (len(s) > 0):
		# ignore whitespaces
		whspace_match = whspace_prog.match(s)
		if (whspace_match):
			s = s[whspace_match.end():]
			continue

		# int literals
		int_lit_match = int_lit_prog.match(s)
		if (int_lit_match):
			int_lit_start = int_lit_match.start()
			int_lit_end = int_lit_match.end()
			int_lit_lexeme = int_lit_match.group(0)

			tok = int_token(
				int(int_lit_end - int_lit_start - 1),
				token_info(i + int_lit_start, i + int_lit_end, int_lit_lexeme)
				)
			tokens.append(tok)

			s = s[int_lit_end:]
			i += (word_end - word_start)
			continue

		# keywords
		word_match = word_prog.match(s)
		if (word_match):
			word_start = word_match.start()
			word_end = word_match.end()
			word_lexeme = word_match.group(0)

			tok = word_token(
				find_kw_token_tag(word_lexeme),
				token_info(i + word_start, i + word_end, word_lexeme)
				)
			tokens.append(tok)

			s = s[word_end:]
			i += (word_end - word_start)
			continue

		# generics
		if (s[0] == "("):
			tok = token(
				token_tags.LP,
				token_info(i, i + 1, "(")
				)
			tokens.append(tok)

			s = s[1:]
			i += 1
			continue

		if (s[0] == ")"):
			tok = token(
				token_tags.RP,
				token_info(i, i + 1, ")")
				)
			tokens.append(tok)

			s = s[1:]
			i += 1
			continue

		# error
		raise Exception("lexer error")

	return tokens

def main(s):
	tokens = lex(s)
	for token in tokens:
		print(token)